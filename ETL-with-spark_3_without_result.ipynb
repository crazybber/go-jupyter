{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "685c6a631c484a989d0281750ef4fa59",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=yarn appName=livy-session-1>"
     ]
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ab45f1d5b4684449af27990db98083a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- -------\n",
      "beautifulsoup4             4.8.1  \n",
      "boto                       2.49.0 \n",
      "cycler                     0.10.0 \n",
      "descartes                  1.1.0  \n",
      "jmespath                   0.9.4  \n",
      "kiwisolver                 1.1.0  \n",
      "lxml                       4.4.2  \n",
      "matplotlib                 3.2.1  \n",
      "mysqlclient                1.4.6  \n",
      "nltk                       3.4.5  \n",
      "nose                       1.3.4  \n",
      "numpy                      1.14.5 \n",
      "pandas                     1.0.3  \n",
      "pip                        20.0.2 \n",
      "py-dateutil                2.2    \n",
      "pyparsing                  2.4.6  \n",
      "pyshp                      2.1.0  \n",
      "python-dateutil            2.8.1  \n",
      "python36-sagemaker-pyspark 1.2.6  \n",
      "pytz                       2019.3 \n",
      "PyYAML                     3.11   \n",
      "setuptools                 46.0.0 \n",
      "Shapely                    1.7.0  \n",
      "six                        1.13.0 \n",
      "soupsieve                  1.9.5  \n",
      "SQLAlchemy                 1.3.15 \n",
      "wheel                      0.34.2 \n",
      "windmill                   1.6"
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 1: load raw data from s3 and do some basic clean and polishing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bc2d908c87c427db8c2079129bc099c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as f\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "import datetime\n",
    "\n",
    "# config spark to build context\n",
    "spark=SparkSession.builder.appName(\"NYC.TLC-Green-Car-Data-ETL\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.session.timeZone\", \"America/New_York\")\n",
    "\n",
    "# read in our raw dataset\n",
    "# add new column :pickup_hour\n",
    "# add new column :dropoff_hour\n",
    "# add new column :duration\n",
    "# add new column :minute_rate\n",
    "# add new column :average_speed\n",
    "\n",
    "# remove colum :store_and_fwd_flag\n",
    "# remove colum :ehail_fee\n",
    "# remove colum :congestion_surcharge\n",
    "\n",
    "df = spark.read.option('header', 'true') \\\n",
    "    .option('mode', 'FAILFAST') \\\n",
    "    .option('timeStampFormat', 'yyyy-MM-dd HH:mm:ss') \\\n",
    "    .option('columnNameOfCorruptRecord', 'error') \\\n",
    "    .csv('s3://data-etl-o-original-raw/green/*.csv') \\\n",
    "    .filter(f.year('lpep_pickup_datetime') == 2019) \\\n",
    "    .withColumn('trip_distance',f.col('trip_distance').cast(DoubleType())) \\\n",
    "    .withColumn('pickup_hour',f.hour('lpep_pickup_datetime')) \\\n",
    "    .withColumn('dropoff_hour',f.hour('lpep_dropoff_datetime')) \\\n",
    "    .withColumn('duration', f.unix_timestamp('lpep_dropoff_datetime') - f.unix_timestamp('lpep_pickup_datetime')) \\\n",
    "    .withColumn('minute_rate',f.when(f.col('total_amount') <= 0.001,0).otherwise(f.col('total_amount')/f.col('duration') * 60)) \\\n",
    "    .withColumn('average_speed',f.when(f.col('trip_distance') < 0.001,0).otherwise(f.col('trip_distance')/f.col('duration') * 60 * 60)) \\\n",
    "    .drop('store_and_fwd_flag') \\\n",
    "    .drop('ehail_fee') \\\n",
    "    .drop('congestion_surcharge')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39b6307ff5c496baf74f1c35a237582",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- minute_rate: double (nullable = true)\n",
      " |-- average_speed: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "365a7a8fee6a4173822b7e7b408e75b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Package already installed for current Spark context!\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/lib/spark/python/lib/pyspark.zip/pyspark/context.py\", line 1110, in install_pypi_package\n",
      "    raise ValueError(\"Package already installed for current Spark context!\")\n",
      "ValueError: Package already installed for current Spark context!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sc.install_pypi_package(\"pandas\")\n",
    "sc.install_pypi_package(\"pyshp\")\n",
    "sc.install_pypi_package(\"shapely\")\n",
    "sc.install_pypi_package(\"descartes\")\n",
    "sc.install_pypi_package(\"sqlalchemy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "637f4376916a45d3882302200543dffd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import random\n",
    "import itertools\n",
    "import math\n",
    "import shapefile\n",
    "from shapely.geometry import Polygon\n",
    "from descartes.patch import PolygonPatch\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 2: cache data im memory for comparing some key properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7668f85a4b546eb9a41e696ead6d860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- minute_rate: double (nullable = true)\n",
      " |-- average_speed: double (nullable = true)\n",
      "\n",
      "root\n",
      " |-- VendorID: string (nullable = true)\n",
      " |-- lpep_pickup_datetime: string (nullable = true)\n",
      " |-- lpep_dropoff_datetime: string (nullable = true)\n",
      " |-- RatecodeID: string (nullable = true)\n",
      " |-- PULocationID: string (nullable = true)\n",
      " |-- DOLocationID: string (nullable = true)\n",
      " |-- passenger_count: string (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- fare_amount: string (nullable = true)\n",
      " |-- extra: string (nullable = true)\n",
      " |-- mta_tax: string (nullable = true)\n",
      " |-- tip_amount: string (nullable = true)\n",
      " |-- tolls_amount: string (nullable = true)\n",
      " |-- improvement_surcharge: string (nullable = true)\n",
      " |-- total_amount: string (nullable = true)\n",
      " |-- payment_type: string (nullable = true)\n",
      " |-- trip_type: string (nullable = true)\n",
      " |-- pickup_hour: integer (nullable = true)\n",
      " |-- dropoff_hour: integer (nullable = true)\n",
      " |-- duration: long (nullable = true)\n",
      " |-- minute_rate: double (nullable = true)\n",
      " |-- average_speed: double (nullable = true)\n",
      "\n",
      "(None, None)"
     ]
    }
   ],
   "source": [
    "dfcache = df.cache()\n",
    "\n",
    "# dfcache\n",
    "## check schema structure\n",
    "df.printSchema(),dfcache.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### step 3: processing further polishing and transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b63192d355a487f9213dfd899f312b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce6c8ab07d3b480c957196797d673bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fix colums with regular names\n",
    "\n",
    "for c in df.columns:\n",
    "    df = df.withColumnRenamed(c, c.replace(' ', '_'))\n",
    "\n",
    "## polish data\n",
    "\n",
    "## remove data which is tooooo small and ilegal value\n",
    "df = df.filter(df['trip_distance'] >= 0.001)\n",
    "\n",
    "## round minute_rate and average_speed to scale 3 \n",
    "\n",
    "df = df.withColumn('minute_rate', f.round(df['minute_rate'],4))\n",
    "\n",
    "df = df.withColumn('average_speed', f.round(df['average_speed'],4))\n",
    "\n",
    "df = df.withColumn('average_speed', f.round(df['trip_distance'],4))\n",
    "\n",
    "# rename 'VendorID'\n",
    "# df = df.withColumnRenamed('VendorID', 'vendor_id')\n",
    "\n",
    "# rename passenger_count to pcount shortly\n",
    "df = df.withColumnRenamed('passenger_count', 'p_count').withColumn('p_count',f.col('p_count').cast(IntegerType())) \n",
    "\n",
    "# df.show(10)\n",
    "\n",
    "df.select(['VendorID', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'duration','total_amount','trip_distance','minute_rate','average_speed']) \\\n",
    ".show(10)\n",
    "\n",
    "## Write Data Back to S3 using AWS API\n",
    "# currenttime = datetime.datetime.now()\n",
    "\n",
    "# df.write.option('compression', 'snappy') \\\n",
    "#    .save( \"s3://data-etl-o-target-0/result/green/green_car_data_processed_{date}/\".format(date=currenttime.strftime('%Y%m%d%H%M%s')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## caculate and comparing the count of the target data \n",
    "\n",
    "df.columns,dfcache.columns\n",
    "\n",
    "df.count(), dfcache.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 4: load taxi zone data from s3 for future merging using"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read in our raw dataset\n",
    "dfz = spark.read.option('header', 'true') \\\n",
    "    .option('mode', 'FAILFAST') \\\n",
    "    .option('columnNameOfCorruptRecord', 'error') \\\n",
    "    .csv('s3://data-etl-o-original-raw/zone/nyc.tlc.taxi_zone_lookup.csv')\n",
    "\n",
    "dfzcache = dfz.cache()\n",
    "\n",
    "# dfz.count(),dfz.columns\n",
    "\n",
    "dfz.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = df.select(['VendorID','PULocationID', 'DOLocationID', 'p_count', 'lpep_pickup_datetime', 'lpep_dropoff_datetime', 'duration','total_amount','trip_distance','minute_rate','average_speed'])\n",
    "tmp.show(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### step 5: handle taxi_zones shape files for further UI visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## download files from internets to prepare data VI\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "## define dir to use\n",
    "nyc_data_dir_path = '/tmp/nyc'\n",
    "\n",
    "if not os.path.exists(nyc_data_dir_path):\n",
    "    os.makedirs(nyc_data_dir_path)\n",
    "\n",
    "target_shapefile_path = '/tmp/nyc/taxi_zones.zip'\n",
    "\n",
    "if not os.path.exists(target_shapefile_path):  \n",
    "    urllib.request.urlretrieve(\"https://s3.amazonaws.com/nyc-tlc/misc/taxi_zones.zip\", target_shapefile_path)\n",
    "\n",
    "target_shapefile_dir = '/tmp/nyc/shape'\n",
    "if not os.path.exists(target_shapefile_dir): \n",
    "    with zipfile.ZipFile(target_shapefile_path,'r') as zip_ref:\n",
    "        zip_ref.extractall(target_shapefile_dir)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "src format:\n",
    "\n",
    "Record #0:[1, 0.116357453189, 0.0007823067885, 'Newark Airport', 1, 'EWR']\n",
    "\n",
    "target format\n",
    "\n",
    "{'OBJECTID': 1, 'Shape_Leng': 0.116357453189, 'Shape_Area': 0.0007823067885, 'zone': 'Newark Airport', 'LocationID': 1, 'borough': 'EWR'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define processing to extract LocationID、longitude、latitude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define proc to extract \"LocationID\", \"longitude\", \"latitude\"\n",
    "\n",
    "def get_lat_lon(sf,shp_dic):\n",
    "    content = []\n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        loc_id = rec[shp_dic['LocationID']]\n",
    "        x = (shape.bbox[0]+shape.bbox[2])/2\n",
    "        y = (shape.bbox[1]+shape.bbox[3])/2\n",
    "        content.append((loc_id, x, y))\n",
    "    return pd.DataFrame(content, columns=[\"LocationID\", \"longitude\", \"latitude\"]).replace(np.NaN, '')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read previous file we download and extracted to /tmp/nyc/shape/\n",
    "sf = shapefile.Reader(\"/tmp/nyc/shape/taxi_zones.shp\")\n",
    "fields_name = [field[0] for field in sf.fields[1:]]\n",
    "shp_dic = dict(zip(fields_name, list(range(len(fields_name)))))\n",
    "attributes = sf.records()\n",
    "shp_attr = [dict(zip(fields_name, attr)) for attr in attributes]\n",
    "\n",
    "df_loc = pd.DataFrame(shp_attr).join(get_lat_lon(sf,shp_dic).set_index(\"LocationID\"), on=\"LocationID\")\n",
    "\n",
    "## df_loc.head()\n",
    "## ported to spark to analyze\n",
    "spark_dfl = spark.createDataFrame(df_loc)\n",
    "spark_dfl.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define processing for draw region map and draw zone map by plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_zone_map(ax, sf, heat={}, text=[], arrows=[]):\n",
    "    continent = [235/256, 151/256, 78/256]\n",
    "    ocean = (89/256, 171/256, 227/256)\n",
    "    theta = np.linspace(0, 2*np.pi, len(text)+1).tolist()\n",
    "    ax.set_facecolor(ocean)\n",
    "    \n",
    "    # colorbar\n",
    "    if len(heat) != 0:\n",
    "        norm = mpl.colors.Normalize(vmin=min(heat.values()),vmax=max(heat.values())) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n",
    "        cm=plt.get_cmap('Reds')\n",
    "        sm = plt.cm.ScalarMappable(cmap=cm, norm=norm)\n",
    "        sm.set_array([])\n",
    "        plt.colorbar(sm, ticks=np.linspace(min(heat.values()),max(heat.values()),8),\n",
    "                     boundaries=np.arange(min(heat.values())-10,max(heat.values())+10,.1))\n",
    "    \n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        loc_id = rec[shp_dic['LocationID']]\n",
    "        zone = rec[shp_dic['zone']]\n",
    "        \n",
    "        if len(heat) == 0:\n",
    "            col = continent\n",
    "        else:\n",
    "            if loc_id not in heat:\n",
    "                R,G,B,A = cm(norm(0))\n",
    "            else:\n",
    "                R,G,B,A = cm(norm(heat[loc_id]))\n",
    "            col = [R,G,B]\n",
    "\n",
    "        # check number of parts (could use MultiPolygon class of shapely?)\n",
    "        nparts = len(shape.parts) # total parts\n",
    "        if nparts == 1:\n",
    "            polygon = Polygon(shape.points)\n",
    "            patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n",
    "            ax.add_patch(patch)\n",
    "        else: # loop over parts of each shape, plot separately\n",
    "            for ip in range(nparts): # loop over parts, plot separately\n",
    "                i0 = shape.parts[ip]\n",
    "                if ip < nparts-1:\n",
    "                    i1 = shape.parts[ip+1]-1\n",
    "                else:\n",
    "                    i1 = len(shape.points)\n",
    "\n",
    "                polygon = Polygon(shape.points[i0:i1+1])\n",
    "                patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n",
    "                ax.add_patch(patch)\n",
    "        \n",
    "        x = (shape.bbox[0]+shape.bbox[2])/2\n",
    "        y = (shape.bbox[1]+shape.bbox[3])/2\n",
    "        if (len(text) == 0 and rec[shp_dic['Shape_Area']] > 0.0001):\n",
    "            plt.text(x, y, str(loc_id), horizontalalignment='center', verticalalignment='center')            \n",
    "        elif len(text) != 0 and loc_id in text:\n",
    "            #plt.text(x+0.01, y-0.01, str(loc_id), fontsize=12, color=\"white\", bbox=dict(facecolor='black', alpha=0.5))\n",
    "            eta_x = 0.05*np.cos(theta[text.index(loc_id)])\n",
    "            eta_y = 0.05*np.sin(theta[text.index(loc_id)])\n",
    "            ax.annotate(\"[{}] {}\".format(loc_id, zone), xy=(x, y), xytext=(x+eta_x, y+eta_y),\n",
    "                        bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12,\n",
    "                        arrowprops=dict(facecolor='black', width=3, shrink=0.05))\n",
    "    if len(arrows)!=0:\n",
    "        for arr in arrows:\n",
    "            ax.annotate('', xy = arr['dest'], xytext = arr['src'], size = arr['cnt'],\n",
    "                    arrowprops=dict(arrowstyle=\"fancy\", fc=\"0.6\", ec=\"none\"))\n",
    "    \n",
    "    # display\n",
    "    limits = get_boundaries(sf)\n",
    "    plt.xlim(limits[0], limits[1])\n",
    "    plt.ylim(limits[2], limits[3])\n",
    "\n",
    "# Draw Borough region\n",
    "\n",
    "def draw_region_map(ax, sf, heat={}):\n",
    "    continent = [235/256, 151/256, 78/256]\n",
    "    ocean = (89/256, 171/256, 227/256)    \n",
    "    \n",
    "    reg_list={'Staten Island':1, 'Queens':2, 'Bronx':3, 'Manhattan':4, 'EWR':5, 'Brooklyn':6}\n",
    "    reg_x = {'Staten Island':[], 'Queens':[], 'Bronx':[], 'Manhattan':[], 'EWR':[], 'Brooklyn':[]}\n",
    "    reg_y = {'Staten Island':[], 'Queens':[], 'Bronx':[], 'Manhattan':[], 'EWR':[], 'Brooklyn':[]}\n",
    "    \n",
    "    # colorbar\n",
    "    if len(heat) != 0:\n",
    "        norm = mpl.colors.Normalize(vmin=math.sqrt(min(heat.values())), vmax=math.sqrt(max(heat.values()))) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n",
    "        cm=plt.get_cmap('Reds')\n",
    "        #sm = plt.cm.ScalarMappable(cmap=cm, norm=norm)\n",
    "        #sm.set_array([])\n",
    "        #plt.colorbar(sm, ticks=np.linspace(min(heat.values()),max(heat.values()),8), \\\n",
    "        #             boundaries=np.arange(min(heat.values())-10,max(heat.values())+10,.1))\n",
    "    \n",
    "    ax.set_facecolor(ocean)\n",
    "    for sr in sf.shapeRecords():\n",
    "        shape = sr.shape\n",
    "        rec = sr.record\n",
    "        reg_name = rec[shp_dic['borough']]\n",
    "        \n",
    "        if len(heat) == 0:\n",
    "            norm = mpl.colors.Normalize(vmin=1,vmax=6) #norm = mpl.colors.LogNorm(vmin=1,vmax=max(heat))\n",
    "            cm=plt.get_cmap('Pastel1')\n",
    "            R,G,B,A = cm(norm(reg_list[reg_name]))\n",
    "            col = [R,G,B]\n",
    "        else:\n",
    "            R,G,B,A = cm(norm(math.sqrt(heat[reg_name])))\n",
    "            col = [R,G,B]\n",
    "            \n",
    "        # check number of parts (could use MultiPolygon class of shapely?)\n",
    "        nparts = len(shape.parts) # total parts\n",
    "        if nparts == 1:\n",
    "            polygon = Polygon(shape.points)\n",
    "            patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n",
    "            ax.add_patch(patch)\n",
    "        else: # loop over parts of each shape, plot separately\n",
    "            for ip in range(nparts): # loop over parts, plot separately\n",
    "                i0 = shape.parts[ip]\n",
    "                if ip < nparts-1:\n",
    "                    i1 = shape.parts[ip+1]-1\n",
    "                else:\n",
    "                    i1 = len(shape.points)\n",
    "\n",
    "                polygon = Polygon(shape.points[i0:i1+1])\n",
    "                patch = PolygonPatch(polygon, facecolor=col, alpha=1.0, zorder=2)\n",
    "                ax.add_patch(patch)\n",
    "                \n",
    "        reg_x[reg_name].append((shape.bbox[0]+shape.bbox[2])/2)\n",
    "        reg_y[reg_name].append((shape.bbox[1]+shape.bbox[3])/2)\n",
    "        \n",
    "    for k in reg_list:\n",
    "        if len(heat)==0:\n",
    "            plt.text(np.mean(reg_x[k]), np.mean(reg_y[k]), k, horizontalalignment='center', verticalalignment='center',\n",
    "                        bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12)     \n",
    "        else:\n",
    "            plt.text(np.mean(reg_x[k]), np.mean(reg_y[k]), \"{}\\n({}K)\".format(k, heat[k]/1000), horizontalalignment='center', \n",
    "                     verticalalignment='center',bbox=dict(facecolor='black', alpha=0.5), color=\"white\", fontsize=12)       \n",
    "\n",
    "    # display\n",
    "    limits = get_boundaries(sf)\n",
    "    plt.xlim(limits[0], limits[1])\n",
    "    plt.ylim(limits[2], limits[3])\n",
    "\n",
    "# get boundaries of zone\n",
    "# %matplotlib inline\n",
    "\n",
    "def get_boundaries(sf):\n",
    "    lat, lon = [], []\n",
    "    for shape in list(sf.iterShapes()):\n",
    "        lat.extend([shape.bbox[0], shape.bbox[2]])\n",
    "        lon.extend([shape.bbox[1], shape.bbox[3]])\n",
    "\n",
    "    margin = 0.01 # buffer to add to the range\n",
    "    lat_min = min(lat) - margin\n",
    "    lat_max = max(lat) + margin\n",
    "    lon_min = min(lon) - margin\n",
    "    lon_max = max(lon) + margin\n",
    "\n",
    "    return lat_min, lat_max, lon_min, lon_max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Borough and zones\n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15,8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.set_title(\"Borough Area in NYC\")\n",
    "draw_region_map(ax, sf)\n",
    "\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.set_title(\"Zones in NYC\")\n",
    "draw_zone_map(ax, sf)\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### for business trending ,we need to exploring some data from merged data\n",
    "\n",
    "### Q1: Which zone have most pickups and drop-offs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## count().orderBy('count', ascending=False).limit(10)\n",
    "\n",
    "df_pu = df.select(f.col(\"PULocationID\").alias(\"LocationID\")).groupby('LocationID').count().withColumnRenamed('count', 'pu_count')\n",
    "df_do = df.select(f.col(\"DOLocationID\").alias(\"LocationID\")).groupby('LocationID').count().withColumnRenamed('count', 'do_count')\n",
    "\n",
    "## use taxi zone talbe\n",
    "joined1 = df_pu.join(df_do, 'LocationID', 'left').withColumn('total_count', df_pu.pu_count + df_do.do_count)\n",
    "\n",
    "joined1.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# order tables\n",
    "joined2 = joined1.join(dfz,'LocationID', 'left') \\\n",
    "    .withColumn(\"LocationID\", f.col('LocationID').cast(IntegerType())) \\\n",
    "    .orderBy('LocationID')\n",
    "\n",
    "joined2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# joined2.select('*').limit(10).show(10)\n",
    "# joinedtemplate = joined2.createOrReplaceTempView(\"joinedtemplate\")\n",
    "#df_putp5 = joined2.orderBy('pu_count', ascending=False).select('*').limit(5)\n",
    "\n",
    "df_putp5 = joined2.orderBy('pu_count', ascending=False).limit(5)\n",
    "df_putp5.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#df_dotp5 = joined2.orderBy('do_count', ascending=False).select(['LocationID','do_count','pu_count','Borough','Zone'])\n",
    "\n",
    "df_dotp5 = joined2.orderBy('do_count', ascending=False).limit(5)\n",
    "\n",
    "df_dotp5.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get UI parameters\n",
    "\n",
    "# [Row(LocationID=74, pu_count=457848), Row(LocationID=75, pu_count=368752), Row(LocationID=41, pu_count=333623)]\n",
    "#list_borough =  [row.LocationID for row in joined2.select(['LocationID','pu_count']).collect()]\n",
    "\n",
    "list_borough = joined2.select(\"borough\").rdd.flatMap(lambda x: x).collect()\n",
    "list_pu_count = joined2.select(\"pu_count\").rdd.flatMap(lambda x: x).collect()\n",
    "list_do_count = joined2.select(\"do_count\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#list_LocationID[1],list_pu_count[1]\n",
    "# get the pick mount from different zone \n",
    "\n",
    "#pu_mount_list = dict(zip(list_borough, list_pu_count))\n",
    "pu_mount_list= joined2.select(['borough','pu_count']).rdd.collectAsMap()\n",
    "\n",
    "list_index_df_putp5 = df_putp5.select(\"LocationID\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "#do_mount_list = dict(zip(list_borough, list_do_count))\n",
    "do_mount_list= joined2.select(['borough','do_count']).rdd.collectAsMap()\n",
    "\n",
    "list_index_df_dotp5 = df_dotp5.select(\"LocationID\").rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "pu_mount_list,do_mount_list,list_index_df_putp5,list_index_df_dotp5"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "list_index_df_putp5 =[row.LocationID for row in df_putp5.select('LocationID').collect()]\n",
    "\n",
    "list_index_df_dotp5 = [row.LocationID for row in df_dotp5.select('LocationID').collect()]\n",
    "\n",
    "list_index_df_putp5,list_index_df_dotp5\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "## way 2\n",
    "df_pu_agg = df.select(f.col(\"PULocationID\").alias(\"LocationID\")).groupBy('LocationID').agg({'LocationID':'count'}).withColumnRenamed('count(LocationID)', 'pu_count')\n",
    "df_do_agg = df.select(f.col(\"DOLocationID\").alias(\"LocationID\")).groupBy('LocationID').agg({'LocationID':'count'}).withColumnRenamed('count(LocationID)', 'do_count')\n",
    "\n",
    "# joinedg1 = df_pu_agg.join(df_do_agg, 'LocationID', 'left').withColumn('total_count', df_pu.pu_count + df_do.do_count)\n",
    "# joinedg2 = joinedg1.join(dfz,'LocationID', 'left')\n",
    "# joinedg2.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(18,8))\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "ax.set_title(\"Boroughs with most pick-ups\")\n",
    "#draw_zone_map(ax, sf, heat = pu_mount_list,text=list_index_df_putp5)\n",
    "draw_region_map(ax, sf, heat = pu_mount_list)\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "ax.set_title(\"Boroughs with most drop-offs\")\n",
    "#draw_zone_map(ax, sf, heat = do_mount_list,text=list_index_df_dotp5)\n",
    "draw_region_map(ax, sf, heat = do_mount_list)\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_pu = pd.read_sql_query('SELECT pickup_hour AS time, count(*) AS PUcount \\\n",
    "                        FROM table_record \\\n",
    "                        GROUP BY pickup_hour', nyc_database)\n",
    "df_do = pd.read_sql_query('SELECT dropoff_hour AS time, count(*) AS DOcount \\\n",
    "                        FROM table_record \\\n",
    "                        GROUP BY dropoff_hour', nyc_database)\n",
    "df_q2 = df_pu.merge(df_do, on=\"time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_do = df.select(f.col(\"DOLocationID\").alias(\"LocationID\")).groupby('LocationID').count().withColumnRenamed('count', 'do_count')\n",
    "\n",
    "df_pu_busy_hour = df.select(f.col('pickup_hour').alias('time')).groupby('time').count().withColumnRenamed('count','pu_count')\n",
    "df_do_busy_hour = df.select(f.col('dropoff_hour').alias('time')).groupby('time').count().withColumnRenamed('count','do_count')\n",
    "\n",
    "df_busy_hours = df_pu_busy_hour.join(df_do_busy_hour, on='time').orderBy('time')\n",
    "\n",
    "df_busy_hours.show(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_hours_time = df_busy_hours.select('time').rdd.flatMap(lambda x: x).collect()\n",
    "list_pu_hours_datas = df_busy_hours.select('pu_count').rdd.flatMap(lambda x: x).collect()\n",
    "list_do_hours_datas = df_busy_hours.select('do_count').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "list_hours_time,list_pu_hours_datas,list_do_hours_datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ax = df_q2.plot(x='time', y=['Pick-ups', 'Drop-offs'], kind='line', style=\"-o\", figsize=(15,1))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12,5))\n",
    "ax.set_title('peak hours in 2019')\n",
    "\n",
    "ax.plot(list_hours_time, list_pu_hours_datas, '-o', label='Pick-ups')\n",
    "\n",
    "ax.plot(list_hours_time, list_do_hours_datas, '-o', label='Drop-offs')\n",
    "\n",
    "ax.set_xlabel('time')\n",
    "ax.set_ylabel('count')\n",
    "\n",
    "\n",
    "ax.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q3: What are the differences between short and long distance trips of taking taxi?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# avoid distance less than 0\n",
    "\n",
    "#df_dist = pd.read_sql_query('SELECT trip_distance FROM table_record WHERE trip_distance > 0', nyc_database)\n",
    "#df_dist['trip_distance'].describe()\n",
    "\n",
    "# get all distance\n",
    "\n",
    "df_trip_distance = df.where(f.col('trip_distance') > 0.0001).select('trip_distance')\n",
    "\n",
    "df_trip_distance.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_distance.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from decimal import Decimal\n",
    "# lambda x: round(Decimal(x),3)\n",
    "\n",
    "list_trip_distance = df_trip_distance.select('trip_distance').rdd.flatMap(lambda x: x).collect()\n",
    "\n",
    "for i in range(10) :\n",
    "    list_trip_distance[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_distance.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fig, ax = plt.subplots(figsize=(15,5))\n",
    "\n",
    "ax.set_title('trip distance (miles)')\n",
    "\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlabel('trip distance (miles)')\n",
    "ax.set_ylabel('total distant count')\n",
    "ax.hist(list_trip_distance,bins=30,label='trip hist')\n",
    "\n",
    "ax.legend()\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_q3_short_count = df.where(f.col('trip_distance') < 30 ).where(f.col('trip_distance') > 0).select('trip_distance').count()\n",
    "\n",
    "df_q3_long_count = df.where(f.col('trip_distance') >= 30 ).select('trip_distance').count()\n",
    "\n",
    "df_q3_short_count,df_q3_long_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_count = df.where(f.col('trip_distance') < 30 ).where(f.col('trip_distance') > 0) \\\n",
    "    .select(['pickup_hour','dropoff_hour']) \\\n",
    "    .groupBy(['pickup_hour','dropoff_hour']) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count','count(short trip)') \\\n",
    "    .withColumnRenamed('pickup_hour','pickup_time') \\\n",
    "    .withColumnRenamed('dropoff_hour','dropoff_time') \n",
    "df_long_count = df.where(f.col('trip_distance') > 30 ) \\\n",
    "    .select(['pickup_hour','dropoff_hour']) \\\n",
    "    .groupBy(['pickup_hour','dropoff_hour']) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count','count(long trip)') \\\n",
    "    .withColumnRenamed('pickup_hour','pickup_time') \\\n",
    "    .withColumnRenamed('dropoff_hour','dropoff_time') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_count.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_short_count = df_short_count.\n",
    "df_trip_hours_theta = df_long_count.join(df_short_count,['pickup_time','dropoff_time'])\n",
    "df_trip_hours_theta.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_hours_theta_pu  = df_trip_hours_theta.groupBy('pickup_time').agg({'count(short trip)': 'sum', 'count(long trip)':'sum'}).orderBy('pickup_time')\n",
    "df_trip_hours_theta_pu.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_hours_theta_do = df_trip_hours_theta.groupBy('dropoff_time').agg({'count(short trip)': 'sum', 'count(long trip)':'sum'}).orderBy('dropoff_time')\n",
    "df_trip_hours_theta_do.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_trip_hours_theta_do.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# render clock time\n",
    "\n",
    "def plt_clock(ax, radii, title, color):\n",
    "    \n",
    "    N = 24\n",
    "    bottom = 2\n",
    "    # create theta for 24 hours\n",
    "    theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)\n",
    "\n",
    "    # width of each bin on the plot\n",
    "    width = (2*np.pi) / N   \n",
    "    bars = ax.bar(theta, radii, width=width, bottom=bottom, color=color, edgecolor=\"#999999\")\n",
    "\n",
    "    # set the lable go clockwise and start from the top\n",
    "    ax.set_theta_zero_location(\"N\")\n",
    "    # clockwise\n",
    "    ax.set_theta_direction(-1)\n",
    "\n",
    "    # set the label\n",
    "    ax.set_xticks(theta)\n",
    "    ticks = [\"{}:00\".format(x) for x in range(24)]\n",
    "    ax.set_xticklabels(ticks)\n",
    "    ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#setup 4 ax plates and plot them\n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18,18))\n",
    "\n",
    "df_trip_count_list = df_trip_hours_theta_pu.select('sum(count(short trip))').rdd.flatMap(lambda x: x).collect()\n",
    "ax = plt.subplot(2,2,1, polar=True)    \n",
    "plt_clock(ax, df_trip_count_list, \"Pickup Time for Short Trips in 2019\", \"#dc143c\")\n",
    "\n",
    "df_trip_count_list = df_trip_hours_theta_pu.select('sum(count(long trip))').rdd.flatMap(lambda x: x).collect()\n",
    "ax = plt.subplot(2,2,2, polar=True)\n",
    "plt_clock(ax, df_trip_count_list, 'Pickup Time for Long Trips in 2019', \"#56B4E9\")\n",
    "\n",
    "df_trip_count_list = df_trip_hours_theta_do.select('sum(count(short trip))').rdd.flatMap(lambda x: x).collect()\n",
    "ax = plt.subplot(2,2,3, polar=True)\n",
    "plt_clock(ax, df_trip_count_list, 'Dropoff Time for Short Trips in 2019', \"#dc143c\")\n",
    "\n",
    "df_trip_count_list = df_trip_hours_theta_do.select('sum(count(long trip))').rdd.flatMap(lambda x: x).collect()\n",
    "ax = plt.subplot(2,2,4, polar=True)\n",
    "plt_clock(ax, df_trip_count_list, 'Dropoff Time for Long Trips in 2019', \"#56B4E9\")\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q4 Which Zones with most pickups for Long or short Trips"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_short_zone = pd.read_sql_query('SELECT PULocationID, DOLocationID, count(*) AS count \\\n",
    "                                 FROM table_record \\\n",
    "                                 WHERE trip_distance < 30 \\\n",
    "                                 GROUP BY PULocationID, DOLocationID', nyc_database)\n",
    "df_long_zone = pd.read_sql_query('SELECT PULocationID, DOLocationID, count(*) AS count \\\n",
    "                                 FROM table_record \\\n",
    "                                 WHERE trip_distance >= 30 \\\n",
    "                                 GROUP BY PULocationID, DOLocationID', nyc_database)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_zone = df.where(f.col('trip_distance') < 30 ).where(f.col('trip_distance') > 0) \\\n",
    "    .select(['PULocationID','DOLocationID']) \\\n",
    "    .groupBy(['PULocationID','DOLocationID']) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count','count(short trip)')\n",
    "df_long_zone = df.where(f.col('trip_distance') >= 30 ) \\\n",
    "    .select(['PULocationID','DOLocationID']) \\\n",
    "    .groupBy(['PULocationID','DOLocationID']) \\\n",
    "    .count() \\\n",
    "    .withColumnRenamed('count','count(long trip)')\n",
    "#    .withColumnRenamed('count','count(short trip)') \\\n",
    "#    .withColumnRenamed('pickup_hour','pickup_time') \\\n",
    "#    .withColumnRenamed('dropoff_hour','dropoff_time') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_zone.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_short_zone.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_zone_counts = df_short_zone \\\n",
    "    .join(df_long_zone,on=['PULocationID', 'DOLocationID']) \\\n",
    "    .join(dfz,f.col('PULocationID') == dfz.LocationID) \\\n",
    "    .drop('service_zone','LocationID') \\\n",
    "    .withColumnRenamed('Borough','From_Borough') \\\n",
    "    .withColumnRenamed('Zone','From_Zone') \n",
    "\n",
    "df_two_zone_counts.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfz.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_zone_all = df_two_zone_counts.join(dfz,df_two_zone_counts.DOLocationID == dfz.LocationID ) \\\n",
    "    .drop('LocationID') \\\n",
    "    .withColumnRenamed('Borough','To_Borough') \\\n",
    "    .withColumnRenamed('Zone','To_Zone') \\\n",
    "    .withColumnRenamed('count(short trip)','short_trip') \\\n",
    "    .withColumnRenamed('count(long trip)','long_trip') \\\n",
    "    .withColumn('PULocationID', f.col('PULocationID').cast(LongType())) \\\n",
    "    .withColumn('DOLocationID', f.col('DOLocationID').cast(LongType()))\n",
    "#df_two_zone_all.show(5)\n",
    "df_two_zone_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_long_tip_top5 = df_two_zone_all.select(['long_trip','From_Zone','To_Zone']).orderBy(f.desc('long_trip'))\n",
    "df_long_tip_top5.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## agg the count between 2 zones\n",
    "df_two_zone_pu = df_two_zone_all.groupBy('PULocationID') \\\n",
    "    .agg({'short_trip':'sum', 'long_trip':'sum'}) \\\n",
    "    .withColumnRenamed('sum(long_trip)','long_trip') \\\n",
    "    .withColumnRenamed('sum(short_trip)','short_trip') \n",
    "df_two_zone_do = df_two_zone_all.groupBy('DOLocationID') \\\n",
    "    .agg({'short_trip':'sum', 'long_trip':'sum'}) \\\n",
    "    .withColumnRenamed('sum(long_trip)','long_trip') \\\n",
    "    .withColumnRenamed('sum(short_trip)','short_trip') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pu_mount_list = dict(zip(list_borough, list_pu_count))\n",
    "df_two_zone_pu_short_dic =df_two_zone_pu.select(['PULocationID','short_trip']).rdd.collectAsMap()\n",
    "df_two_zone_pu_long_dic =df_two_zone_pu.select(['PULocationID','long_trip']).rdd.collectAsMap()\n",
    "\n",
    "df_two_zone_do_short_dic =df_two_zone_do.select(['DOLocationID','short_trip']).rdd.collectAsMap()\n",
    "df_two_zone_do_long_dic =df_two_zone_do.select(['DOLocationID','long_trip']).rdd.collectAsMap()\n",
    "\n",
    "#df_two_zone_pu_short_dic,df_two_zone_pu_long_dic,df_two_zone_do_short_dic,df_two_zone_do_long_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_two_zone_pu.printSchema()\n",
    "#shp_dic\n",
    "#df_two_zone_pu_short_dic\n",
    "#,df_two_zone_pu_long_dic,df_two_zone_do_short_dic,df_two_zone_do_long_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top 5 data list to plot\n",
    "df_two_zone_pu_short = df_two_zone_pu.orderBy(f.desc('short_trip')).limit(5)\n",
    "df_two_zone_pu_long = df_two_zone_pu.orderBy(f.desc('long_trip')).limit(5)\n",
    "\n",
    "# get top 5 data list to plot\n",
    "df_two_zone_do_short = df_two_zone_do.orderBy(f.desc('short_trip')).limit(5)\n",
    "df_two_zone_do_long = df_two_zone_do.orderBy(f.desc('long_trip')).limit(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PUcount_short = dict(zip(df_q3_PU['PULocationID'].tolist(), df_q3_PU['short trips'].tolist()))\n",
    "\n",
    "df_two_zone_pu_short_index_list = df_two_zone_pu_short.select('PULocationID').rdd.flatMap(lambda x : x).collect()\n",
    "df_two_zone_pu_long_index_list = df_two_zone_pu_long.select('PULocationID').rdd.flatMap(lambda x : x).collect()\n",
    "\n",
    "df_two_zone_do_short_index_list = df_two_zone_do_short.select('DOLocationID').rdd.flatMap(lambda x : x).collect()\n",
    "df_two_zone_do_long_index_list = df_two_zone_do_long.select('DOLocationID').rdd.flatMap(lambda x : x).collect()\n",
    "\n",
    "df_two_zone_pu_short_index_list,df_two_zone_pu_long_index_list,df_two_zone_do_short_index_list,df_two_zone_do_long_index_list"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_two_zone_do_short_index_list_manhattan = df_two_zone_do_short.select('DOLocationID') \\\n",
    "    .where((f.col('From_Borough') == 'Manhattan') | \n",
    "           (f.col('To_Borough') === 'Manhattan') ) \\\n",
    "    .rdd.flatMap(lambda x : x).collect()\n",
    "df_two_zone_do_long_index_list_manhattan = df_two_zone_do_long.select('DOLocationID') \\\n",
    "    .where((f.col('From_Borough') == 'Manhattan') | \n",
    "           (f.col('To_Borough') === 'Manhattan') ) \\\n",
    "    .rdd.flatMap(lambda x : x).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot data \n",
    "\n",
    "fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(18,18))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.set_title(\"Zones with most pickups for Short Trips\")\n",
    "draw_zone_map(ax, sf, heat=df_two_zone_pu_short_dic, text=df_two_zone_pu_short_index_list)\n",
    "#draw_region_map(ax, sf, heat=df_two_zone_pu_short_dic)\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.set_title(\"Zones with most pickups for Long Trips\")\n",
    "draw_zone_map(ax, sf, heat=df_two_zone_pu_long_dic, text=df_two_zone_pu_long_index_list)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.set_title(\"Zones with most drop-offs for Short Trips\")\n",
    "draw_zone_map(ax, sf, heat=df_two_zone_do_short_dic, text=df_two_zone_do_short_index_list)\n",
    "ax = plt.subplot(2, 2, 4)\n",
    "ax.set_title(\"Zones with most drop-offs for Long Trips\")\n",
    "draw_zone_map(ax, sf, heat=df_two_zone_do_long_dic, text=df_two_zone_do_long_index_list)\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this exploring more \n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filterAndCaculateTargetFields(target_field_attr):    \n",
    "    #handle short trip \n",
    "    #.where(f.col('p_count') >=1 ) \\\n",
    "    df_short_trips_groups = df.where(f.col('trip_distance') < 30 ).where(f.col('trip_distance') > 0) \\\n",
    "            .select(target_field_attr) \\\n",
    "            .groupBy(target_field_attr) \\\n",
    "            .agg({target_field_attr:'sum'}) \\\n",
    "            .withColumnRenamed('sum('+ target_field_attr +')','short_trip_count') \n",
    "\n",
    "    # hndle long trip\n",
    "    # filter target trip and begain to handle long trip\n",
    "    df_long_trips_groups = df.where(f.col('trip_distance') >= 30 ) \\\n",
    "        .select(target_field_attr,'trip_distance') \\\n",
    "        .groupBy(target_field_attr).agg({target_field_attr:'sum','trip_distance':'avg'}) \\\n",
    "        .withColumnRenamed('avg(trip_distance)','av_trip_distance') \\\n",
    "        .withColumnRenamed('sum('+ target_field_attr +')','long_trip_count') \n",
    "\n",
    "    # merge tables\n",
    "    df_trips_groups_merged = df_short_trips_groups.join(df_long_trips_groups,on=target_field_attr).orderBy(target_field_attr)\n",
    "\n",
    "    short_trips_sum = df_trips_groups_merged.select('short_trip_count').rdd.map(lambda x:x[0]).reduce(lambda x,y: x + y)\n",
    "    long_trips_sum= df_trips_groups_merged.select('long_trip_count').rdd.map(lambda x:x[0]).reduce(lambda x,y: x + y)\n",
    "    #short_trips_sum,long_trips_sum\n",
    "    #df_sum_short_trip = df_short_trips_group_merged.withColumn('rate_short_trip_count',f.col('short_trip_count')/f.sum('short_trip_count'))\n",
    "\n",
    "    df_trips_groups_merged_all = df_trips_groups_merged \\\n",
    "        .withColumn('rate_of_short_trip',f.col('short_trip_count')/short_trips_sum)\\\n",
    "        .withColumn('rate_of_long_trip',f.col('long_trip_count')/long_trips_sum) \\\n",
    "        .withColumn('av_trip_distance', f.round(f.col('av_trip_distance'),4))  \\\n",
    "        .withColumn('rate_of_short_trip', f.round(f.col('rate_of_short_trip'),3))  \\\n",
    "        .withColumn('rate_of_long_trip', f.round(f.col('rate_of_long_trip'),3)) \n",
    "    return df_trips_groups_merged_all\n",
    "\n",
    "df_trips_groups_merged_all = filterAndCaculateTargetFields('p_count')\n",
    "\n",
    "#df_trips_target_data_table.show(10)\n",
    "df_trips_groups_merged_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "## here we can create a pie\n",
    "df_trips_groups_merged_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#long_trip_count,short_trip_count we can create a pie plot\n",
    "#short_trips_sum,long_trips_sum\n",
    "df_trips_groups_merged_all.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diff_short_long_trip_on(attr_field,df_trips_groups_merged_all):\n",
    "\n",
    "    x_axis_label_list = df_trips_groups_merged_all.select(attr_field).rdd.map(lambda x:x[0]).collect()\n",
    "    y_axis_short_trip_rate_list = df_trips_groups_merged_all.select('rate_of_short_trip').rdd.map(lambda x:x[0]).collect()\n",
    "    y_axis_long_trip_rate_list = df_trips_groups_merged_all.select('rate_of_long_trip').rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,5))\n",
    "\n",
    "    width = 0.35  # the width of the bars\n",
    "\n",
    "    x = np.arange(len(x_axis_label_list))  # the label locations\n",
    "\n",
    "    ax.set_ylabel('rate of trip distance')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(x_axis_label_list)\n",
    "    ax.set_title(attr_field.replace('_', ' ')+' difference in short/long trip')\n",
    "\n",
    "    rects1 = ax.bar(x - width/2, y_axis_short_trip_rate_list, width, label='short trip')\n",
    "    rects2 = ax.bar(x + width/2, y_axis_long_trip_rate_list, width, label='long trip')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    def stamplabel(rects):\n",
    "        for rect in rects:\n",
    "            height = rect.get_height()\n",
    "            ax.annotate('{}'.format(height), xy=(rect.get_x() + rect.get_width() / 2, height), textcoords=\"offset points\", ha='center')\n",
    "\n",
    "    stamplabel(rects1)\n",
    "    stamplabel(rects2)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "diff_short_long_trip_on('p_count',df_trips_groups_merged_all)\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=Standard rate\n",
    "# 2=JFK\n",
    "# 3=Newark\n",
    "# 4=Nassau or Westchester\n",
    "# 5=Negotiated fare\n",
    "# 6=Group ride\n",
    "\n",
    "tmpt = filterAndCaculateTargetFields('RatecodeID')\n",
    "diff_short_long_trip_on('RatecodeID',tmpt)\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1=Credit card\n",
    "# 2=Cash\n",
    "# 3=No charge\n",
    "# 4=Dispute\n",
    "\n",
    "tmpt = filterAndCaculateTargetFields('payment_type')\n",
    "diff_short_long_trip_on('payment_type',tmpt)\n",
    "%matplot plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q5 Explorer average speed between citys with some complicate logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .withColumnRenamed('count','count(short trip)')\n",
    "    \n",
    "df_zone_statics = df.where(f.col('trip_distance') > 0.001) \\\n",
    "    .select(['PULocationID','DOLocationID','average_speed','minute_rate']) \\\n",
    "    .groupBy(['PULocationID','DOLocationID']) \\\n",
    "    .agg(f.avg('average_speed'),f.avg('minute_rate')) \\\n",
    "    .withColumn('avg(average_speed)',f.round('avg(average_speed)',3)) \\\n",
    "    .withColumn('avg(minute_rate)',f.round('avg(minute_rate)',3)) \\\n",
    "    .withColumnRenamed('avg(average_speed)','average_speed') \\\n",
    "    .withColumnRenamed('avg(minute_rate)','minute_rate') \n",
    "\n",
    "df_zone_statics.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone_speed_statics_all_1  = dfz.join(df_zone_statics,f.col('LocationID') == df_zone_statics.PULocationID) \\\n",
    "    .drop('service_zone','LocationID','PULocationID') \\\n",
    "    .withColumnRenamed('Borough','From_Borough') \\\n",
    "    .withColumnRenamed('Zone','From_Zone') \n",
    "\n",
    "#df_zone_speed_statics_all_1.show(10)\n",
    "\n",
    "df_zone_speed_statics_all = dfz.join(df_zone_speed_statics_all_1,f.col('LocationID') == df_zone_speed_statics_all_1.DOLocationID) \\\n",
    "    .drop('service_zone','LocationID','DOLocationID') \\\n",
    "    .withColumnRenamed('Borough','To_Borough') \\\n",
    "    .withColumnRenamed('Zone','To_Zone') \n",
    "\n",
    "#    .withColumn('PULocationID', f.col('PULocationID').cast(LongType())) \\\n",
    "#    .withColumn('DOLocationID', f.col('DOLocationID').cast(LongType()))\n",
    "\n",
    "df_zone_speed_statics_all.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone_speed_statics_bro = df_zone_speed_statics_all.groupBy(['To_Borough','From_Borough']).agg({'average_speed':'avg','minute_rate':'avg'}) \\\n",
    "    .withColumn('avg(average_speed)',f.round('avg(average_speed)',3)) \\\n",
    "    .withColumn('avg(minute_rate)',f.round('avg(minute_rate)',3)) \\\n",
    "    .withColumnRenamed('avg(average_speed)','average_speed') \\\n",
    "    .withColumnRenamed('avg(minute_rate)','minute_rate') \n",
    "    \n",
    "df_zone_speed_statics_bro.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_avg_speeds_sum = df_zone_speed_statics_bro.select('average_speed').rdd.map(lambda x:x[0]).reduce(lambda x,y: x + y)\n",
    "\n",
    "df_zone_speed_statics_tags10 = df_zone_speed_statics_bro.withColumn('driving_direction',f.concat_ws( ' To ','From_Borough','To_Borough')) \\\n",
    "    .withColumn('speed_rate',f.col('average_speed')/all_avg_speeds_sum) \\\n",
    "    .withColumn('speed_rate', f.round(f.col('speed_rate'),4)) \\\n",
    "    .orderBy(f.desc('average_speed')).select('*').limit(10)\n",
    "\n",
    "df_zone_speed_statics_tags10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_zone_speed_statics_tags10.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_avg_speeds_sum_top10 = df_zone_speed_statics_tags10.select('average_speed').rdd.map(lambda x:x[0]).reduce(lambda x,y: x + y)\n",
    "\n",
    "df_zone_speed_statics_top10_size_list = df_zone_speed_statics_tags10.select('average_speed').rdd.map(lambda x:x[0]).collect()\n",
    "df_zone_speed_statics_top10_tags_list  = df_zone_speed_statics_tags10_rank.select(['driving_direction']).rdd.map(lambda x:x[0]).collect()\n",
    "\n",
    "df_zone_speed_statics_top10_size_list,df_zone_speed_statics_top10_tags_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1, ax1 = plt.subplots(figsize=(15,9))\n",
    "\n",
    "#explode =  np.arange(len(df_zone_speed_statics_top10_size_list))  # the label locations\n",
    "\n",
    "explode =  [x* 0 for x in range(len(df_zone_speed_statics_top10_size_list))]  # the label locations\n",
    "\n",
    "explode[0] = 0.1 # set explode 1st\n",
    "\n",
    "wedges, texts, autotexts = ax1.pie(df_zone_speed_statics_top10_size_list, \n",
    "        explode=explode, \n",
    "        labels=df_zone_speed_statics_top10_tags_list, \n",
    "        autopct='%2.2f%%',\n",
    "        shadow=True, startangle=90)\n",
    "\n",
    "ax1.set_title(\"Top 10 Speed Between Broughs as PIE\")\n",
    "\n",
    "plt.axis('equal') # # Equal aspect ratio ensures that pie is drawn as a circle.\n",
    "\n",
    "ax1.legend(wedges, df_zone_speed_statics_top10_tags_list,\n",
    "          title=\"Ingredients\",\n",
    "          loc=\"center left\",\n",
    "          bbox_to_anchor=(-0.15, 0.3, 0.5, 1))\n",
    "\n",
    "plt.show()\n",
    "\n",
    "%matplot plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## that is all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 2
   },
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
